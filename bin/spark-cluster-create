#!/usr/bin/env python

from dtde import clusterlib, util
import argparse 
import os

try:
    import configparser
except ImportError:
    import ConfigParser as configparser

# Path to config
CONFIG_PATH = os.path.join(os.path.dirname(__file__), '../configuration.cfg') 

if __name__ == '__main__':

    pool_id = None
    vm_count = None
    vm_size = None
    custom_script = None
    wait = True

    # parse arguments
    parser = argparse.ArgumentParser(prog='az_spark')

    parser.add_argument('--id', dest='cluster_id', required=True,
                        help='The unique id of your spark cluster')
    parser.add_argument('--size', type=int, required=True,
                        help='Number of vms in your cluster')
    parser.add_argument('--vm-size', required=True,
                        help='VM size for nodes in your cluster')
    parser.add_argument('--custom-script', 
                        help='Absolute path of custom bash script (.sh) to run on each node')
    parser.add_argument('--wait', dest='wait', action='store_true')
    parser.add_argument('--no-wait', dest='wait', action='store_false')
    parser.set_defaults(wait=False)

    args = parser.parse_args()
    
    if args.cluster_id is not None:
        pool_id = args.cluster_id

    if args.size is not None:
        vm_count = args.size

    if args.vm_size is not None:
        vm_size = args.vm_size
 
    if args.custom_script is not None:
        custom_script = args.custom_script
 
    if args.wait is not None:
        if args.wait == False:
            wait = False

    print('-------------------------------------------')
    print('spark cluster id:        {}'.format(pool_id))
    print('spark cluster size:      {}'.format(vm_count))
    print('spark cluster vm size:   {}'.format(vm_size))
    print('path to custom script:   {}'.format(custom_script))
    print('wait for cluster:        {}'.format(wait))
    print('-------------------------------------------')

    # Read config file
    global_config = configparser.ConfigParser()
    global_config.read(CONFIG_PATH)

    # Set up batch configuration
    batch_account_key = global_config.get('Batch', 'batchaccountkey')
    batch_account_name = global_config.get('Batch', 'batchaccountname')
    batch_service_url = global_config.get('Batch', 'batchserviceurl')

    # Set up storage configuration
    storage_account_key = global_config.get('Storage', 'storageaccountkey')
    storage_account_name = global_config.get('Storage', 'storageaccountname')
    storage_account_suffix = global_config.get('Storage', 'storageaccountsuffix')

    # create batch client
    batch_client = util.create_batch_client(
            batch_account_key,
            batch_account_name,
            batch_service_url)

    # create storage client
    blob_client = util.create_blob_client(
            storage_account_key,
            storage_account_name,
            storage_account_suffix)

    # create spark cluster
    clusterlib.create_cluster(
        batch_client,
        blob_client,
        custom_script,
        pool_id,
        vm_count,
        vm_size, 
        wait)
