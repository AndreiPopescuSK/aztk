#!/usr/bin/env python

from dtde import clusterlib, util, azure_api
import argparse 
import os

if __name__ == '__main__':

    pool_id = None
    vm_count = 0
    vm_low_pri_count = 0
    vm_size = None
    custom_script = None
    wait = True
    username = None
    password = None

    # parse arguments
    parser = argparse.ArgumentParser(prog='az_spark')

    parser.add_argument('--id', dest='cluster_id', required=True,
                        help='The unique id of your spark cluster')

    # Make --size and --size-low-pri mutually exclusive until there is a fix for
    # having clusters with mixed priority types
    size_group = parser.add_mutually_exclusive_group(required=True)
    size_group.add_argument('--size', type=int,
                        help='Number of vms in your cluster')
    size_group.add_argument('--size-low-pri', type=int,
                        help='Number of low priority vms in your cluster')
    parser.add_argument('--vm-size', required=True,
                        help='VM size for nodes in your cluster')
    parser.add_argument('--custom-script', 
                        help='Absolute path of custom bash script (.sh) to run on each node')
    parser.add_argument('--username',
                        help='Username to access your cluster (required: --wait flag)')
    parser.add_argument('--password',
                        help='Password to access your cluster (required: --wait flag)')
    parser.add_argument('--no-wait', dest='wait', action='store_false')
    parser.add_argument('--wait', dest='wait', action='store_true')
    parser.set_defaults(wait=False)


    args = parser.parse_args()

    if args.username is not None and \
        args.password is not None and \
        args.wait is False:
        print('You must specify set --wait to "True" ' +
              'in order to specify a username/password.')
        exit()

    if (args.username is not None and args.password is None) or \
        (args.username is None and args.password is not None):
        print('You must specify both --username and --password together')
        exit()

    if args.cluster_id is not None:
        pool_id = args.cluster_id

    if args.size is not None:
        vm_count = args.size

    if args.size_low_pri is not None:
        vm_low_pri_count = args.size_low_pri

    if args.vm_size is not None:
        vm_size = args.vm_size
 
    if args.custom_script is not None:
        custom_script = args.custom_script
 
    if args.wait is not None:
        if args.wait == False:
            wait = False
    
    if args.username is not None:
        username = args.username

    if args.password is not None:
        password = args.password

    print('-------------------------------------------')
    print('spark cluster id:        {}'.format(pool_id))
    print('spark cluster size:      {}'.format(vm_count + vm_low_pri_count))
    print('>        dedicated:      {}'.format(vm_count))
    print('>     low priority:      {}'.format(vm_low_pri_count))
    print('spark cluster vm size:   {}'.format(vm_size))
    print('path to custom script:   {}'.format(custom_script))
    print('wait for cluster:        {}'.format(wait))
    print('username:                {}'.format(username))
    print('password:                {}'.format(password))
    print('-------------------------------------------')

    # create spark cluster
    clusterlib.create_cluster(
        azure_api.get_batch_client(),
        azure_api.get_blob_client(),
        custom_script,
        pool_id,
        vm_count,
        vm_low_pri_count,
        vm_size, 
        username,
        password,
        wait)
